% This file is part of the TargetSelection project
% Copyright 2019 the authors.

% To-dos
% ------
% - make a to-do list
% - make style notes
% - make sure the document structure is related to the mind map (email 2019-08-13)
% - Title okay?
% - ...

% Style notes
% - 

\documentclass[modern]{aastex62}
% \usepackage{graphicx, xcolor}
% \usepackage[sort&compress]{natbib}
\usepackage{amsmath}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\pc}{\unit{pc}}
\newcommand{\kpc}{\unit{kpc}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\given}{\,|\,}
\newcommand{\set}[1]{\left\{{#1}\right\}}

% chemical macros
\newcommand{\abundance}[2]{\mathrm{\left[{#1}/{#2}\right]}}
\newcommand{\alphafe}{\abundance{\alpha}{Fe}}

% text macros
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}
\newcommand{\appendixnames}{Appendices}
\newcommand{\equationname}{equation}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et al.}}
\newcommand{\etc}{\foreign{etc.}}
\renewcommand{\paragraph}[1]{\medskip\noindent\textit{#1} ---}

% margins and headers and spacing changes
\renewcommand{\twocolumngrid}{}  % trust in Hogg -- bibliography fix
\setlength{\parindent}{1.4em}    % trust in Hogg -- make indents look like squares
\addtolength{\topmargin}{-0.4in} % trust in Hogg -- taller page
\addtolength{\textheight}{0.7in} % trust in Hogg -- taller page
\shorttitle{requirements for source selection}
\shortauthors{hogg \& rix}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
% \graphicspath{ {figures/} }
%\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{\textbf{%
Source selection for catalogs and spectroscopic follow-up:\\
General requirements for statistical and legacy value%
}}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}
\affil{Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}

\author[0000-0003-4996-9069]{Hans-Walter Rix}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Many new large cataloging and mapping projects are being built and operated
in the areas of
large-scale structure, Milky Way cartography, stellar physics,
exoplanet discovery, and explosive transients.
These must select sources from imaging (and other) data for inclusion
in the catalog or study, or for spectroscopic follow-up observation.
These projects generally want their results to be useful for
statistics, population inferences, and legacy use in future projects.
% aims
Here we lay out some general principles for source selection that will
create this legacy value.
% methods
The key idea, from which all principles flow, is that the survey must
be able to produce a computationally tractable
likelihood function for parameters of interest.
This, in turn, recommends that selection be performed such that it is possible
to construct a relatively simple selection function, or a probability
with which any source (real or counter-factual) would have been selected
in a repeat of the survey.
% results
Among our findings and recommendations are the following:
You want to base your selection as much as possible on quantities that
can be reliably and straightforwardly predicted by the relevant
physical models.
You want to avoid having the selection of one source
depend strongly on the properties of \emph{other} sources.
You don't need to know everything about how sources were selected, you
just need to know those aspects of selection that project most strongly
onto the matters of great interest.
If you are presented with survey data that were selected badly,
it is still possible to make a conditional likelihood function that can be
used for inference, but at information-theoretic cost.
\end{abstract}

\keywords{\raggedright
 catalogs
 ---
 methods:~observational
 ---
 methods:~statistical
 ---
 surveys
 ---
 techniques:~spectroscopic
 ---
 telescopes
}

\section*{}\clearpage
\section{Why source selection matters}\label{sec:intro}

Let's start with an example:
Imagine that you want to learn the exponential scale length of the
Milky Way disk.
One approach is to take spectroscopy of stars in some region or box in
color--magnitude space in some region or fields on the sky,
work out their distances using spectroscopically determined absolute
magnitudes, that is, spectroscopic
parallaxes.
We will then perform inference on that catalog.
But of course the distribution of stars in the sample or catalog you have produced will not
follow the exponential distribution expected from a pure exponential-disk
model.
This is for two reasons:
The first is that your measurements of distance will be noisy.
The second---and far more important---is that your selection of stars in
some color-magnitude box enormously affects the distribution of stars
you actually get in your sample.
This is a classic version of a \emph{selection effect} and, if you
have designed your experiment sensibly, it can be described in terms of a
\emph{selection function}.

More formally, let's imagine that your model of the Milky Way says
that the true properties $z$ of stars should be distributed
according to some true density function $\lambda_0(z\given\theta)$,
where $\theta$ is a set of parameters describing the physical model,
for example the scale length, scale height, and overall stellar mass
of the Milky Way disk.
The properties $z$ might be foremost true Galactocentric position
(possibly translated to heliocentric coordinates) and true luminosity,
but also perhaps true metallicities, temperatures, and so on.
The density function $\lambda_0(z\given\theta)$ delivers the expected
number density of stars in the Milky Way (number per $z$-space or
property-space volume) as a function of $z$.

The whole experiment results in a catalog of $N$ objects,
$\set{y_n}_{n=1}^N$, where each $y_n$ contains the data available on
each object, comprising in our example both estimates of physical
quantities derived from the spectra (spectroscopic parallax, effective
temperature, radial velocity, \etc) and the information used in making
the decision whether to target the object in the first place
(position, magnitude, color, \etc).
The catalog entry $y_n$ may also contain quantities that neither
entered the targeting decision nor were derived from the spectra, but
may prove useful in the subsequent analysis, such as proper motions.
The density of stars $\lambda(y\given\theta)$ you expect to get
in the catalog in data space
is very different from the true density $\lambda_0(z\given\theta)$.
The most obvious and important difference is that many (real or
counter-factual) objects are not in the catalog as they were not targeted,
because they were too faint, too blue, or outside the survey area.
Another difference is that this catalog density
is a function not of true stellar properties $z$ but rather of the noisily
observed properties $y$.

This catalog density function $\lambda(y\given\theta)$ delivers the expected number
density of stars in your catalog (number per $y$-space or observed-data volume)
as a function of observed data $y$.
This rate function is a product of the true density $\lambda_0(\cdot)$
times a \emph{selection function} $S(y)$ which describes the
probability that the source would have entered the catalog:
\begin{equation}
\lambda(y\given\theta) = S(y)\,\int\lambda_0(z\given\theta)\,p(y\given z)\,\dd z
\quad,
\end{equation}
where $p(y\given z)$ is a likelihood function, or the probability
density that you would observe data $y$ if the star has true
properties $z$, and the integral is over the whole possible domain of
true properties $z$.
The argument of $S(y)$ is all observed properties $y$; in practice,
$S(y)$ may not depend at all on many of them, which could then be
dropped from explicit consideration.
In the simple illustrative case at hand $S(y)$ may effectively only be
a function only of position and photometry, even though $y$ contains
many more things.
The likelihood function or noise model $p(y\given z)$ corrects the true density function
for the fact that the observations are noisy; the integral makes this a
kind of convolution.
The selection function $S(y)$ corrects the density function for the
selection effects.
Inference of the parameters $\theta$ of the Galaxy proceeds by
building a model of your observed catalog in the context of this
convolved, selected rate $\lambda(y\given\theta)$ in the data space
$y$.

Our goal in this \documentname\ is to discuss the conditions under
which this selection function can be well understood in a real survey.
It turns out that if you want this selection function to be both
accurately known and simple to operate, there are rules that a project
must follow.

Many projects in conteporary astrophysics proceed in this manner:
Obtain wide-field imaging; select targets for spectroscopic follow-up
in some color--magnitude region; and perform inferences of planet,
star, supernova, galaxy, or quasar populations.
This mode is very old, but it became truly data-intensive with
the \project{Sloan Digital Sky Survey} (\citealt{sdss})
and \project{2dF} (\citealt{2df})
projects, which mapped the large-scale structure to test and learn the
cosmological model.
Now there are projects with this character in a range of different
astrophysics sub-fields.
The successes have been legion:
We know the parameters of the cosmological model at percent-level
precision, supernovae of many types have been found and monitored,
hundreds of exoplanets have been spectroscopically confirmed,
and we have three-dimensional maps of the Milky Way in
element abundances and kinematics.
In this \documentname, we would like to emphasize the point that the
quality of the results from these surveys depends both on the
spectroscopic data they take, and equally (or even more importantly)
on the way the spectroscopic targets were selected from the original
imaging data.

Once a project is building a catalog with thousands to millions of
entries, the end-to-end uncertainties in the results it produces tend
to be dominated not by the shot noise (square-root-of-$N$) or sample
size, but rather the ability to understand the selection effects.
That is, to make precise measurements of a population of galaxies or
stars or planets, we need to know what galaxies or stars or planets
were and were not selected, and what would
have been selected differently if the population of interest had been
different.
That is, we need to understand the \emph{selection function} of the
survey.
We will say more about this below, but the selection function is,
loosely, the probability that a source on the sky would have been
included, conditioned on the
position and physical or observational properties of that source.
The object of this \documentname\ is to elucidate the aspects of
source selection are most relevant to understanding and obtaining a
computationally tractable model of the selection function.

Even when a project is beautifully designed and
executed, it isn't always possible to understand or model this
selection function accurately.
For example, when sources are selected by hand or by
a human process or including a human classification or vetting, it is often difficult to
figure out what would or would not have been included in different
(counter-factual) situations.
For another, when each source is chosen in a way that depends strongly
on properties of that source that are difficult to understand within
the context of the physical model, it is difficult to build a useful
model of the selection function.
It might be accurate, but not useful!
For another, when one source's selection depends on the properties
of other sources in the vicinity of that source, the selection
function might depend on spatial structures that are hard to model or
hard to manipulate computationally.
For yet another, when a survey is beautifully selected \foreign{a priori},
but has low
\foreign{a posteriori} success rate on follow-up observations that
are required for inclusion in the catalog, the selection
function might depend on properties of the sources that are hard to
understand or model.

In this \documentname, we develop guidelines for source selection and
catalog generation
that, if followed, will improve the chances that it will be possible
to build a simple and accurate selection function model.
This, in turn, will improve the chances that the resulting data
will be useful for building a precise model of the relevant source
population.
This, in turn, increases the value of the project for its investigators,
and the legacy value for the community.

In what follows, we make some assumptions about what's important,
scientifically, about the project of interest, and what is not.
Full disclosure: We are motivated by our role in helping to design the
\project{SDSS-V} Project, which will take millions of spectra of
active galaxies in the Universe and stars in the Milky Way
(\citealt{sdssv}).
We will assume that, like in \project{SDSS-V}, the investigators
making or using the catalog or project of interest want to understand
statistical properties of the objects under study.
For example, the mean density of galaxies, or the fraction of galaxies
of different types in different environments;
or the relative rates of different types of supernovae;
or the density of stars as a function of chemical abundances and radius in the Milky Way disk;
or the rate at which stars host planets in a certain mass and period range.

We will assume something a bit stronger than this, even:
We will assume that the project cares about these goals more,
or equally with, discovery rate or sample purity.
That is, the rules that we elucidate will put (at least slightly)
legacy statistical value in competition with discovery rate or sample
purity.
This is because the statistical considerations will put restrictions
on what can be used for selection, and those restrictions will come at
the cost of some opportunities to improve purity.
We will also assume (in this \documentname) that the survey is performing
its sample selection in a single pass, not adaptively, using some
targetted observations, using these to refine the target sample, and
iterating observationally.
This makes our discussion less useful to those spectroscopically following up exoplanet
candidates and supernovae, the characterization of which evolves with
the number of spectroscopic epochs.
It is possible to generalize what's presented here to this time-dependent, adaptive
case, but it is out of scope for this \documentname.
Finally, we will not address special issues that come in to play in the
measurement of 2-point functions (like correlation functions and power
spectra) or higher-order spatial statistics.
The measurement of these spatial statistics bring additional issues
that are also out of scope here.

\section{Scientific goals: A toy example}\label{sec:toy}

HOGG: Describe the fake goal

HOGG: Describe the fake data

HOGG: Describe the selection function

HOGG: Note the independence assumption and its technical wrongness.

\section{The selection function}\label{sec:sf}

We'd like to select our sources and operate our survey or project such
that it is possible to construct a computationally tractable and
relatively accurate model of the selection function $S$, or the
probability that a source would have been selected for the catalog, as
a function of that source's properties.
The reason that this is our goal---the reason that we want a selection
function---is that we need it to perform population-level inferences.
More technically, we need it to construct a likelihood function, as we
argue (outside the main text)
in \appendixname~\ref{app:lf}, and we need that likelihood
function in order to make information-theory-optimal inferences.

But even if we don't want to be information-theory optimal: The
selection function is the ratio of what is observed to the whole
population, so any inference about or reconstruction of the whole
population will require it.
That is, we will either multiply our theory of the world by this
selection function, or we will divide our data by it, before we
compare theory to data.
One of these (the former) is much to be preferred over the other, as
we will argue in \sectionname~\ref{sec:howto}, but either way we
need the selection function.

In what follows, every source $n$ we could observe has two kinds of
properties.
One is its true properties $z_n$, which are latent
properties (technically unobservable), and the other are its observed
properties $y_n$, which are the instrument readings or outputs on that
source, if such outputs are or were made.
In general $y_n$ and $z_n$ will be vectors or blobs of properties.
For concreteness, you could think of the $z_n$ as being the true
temperature, surface gravity, composition, and position (in phase
space) of a star, and the $y_n$ as the measured brightness of the star
in various passbands, plus maybe measured parallax and proper motion.
Or, even simpler, the $z_n$ could be the true brightnesses that would
have been measured with much better measurements (perfect data), and
the $y_n$ are the actual measurements we got of those brightnesses.

In principle, the appearance of a source in our catalog can depend
on either or both of these things (true and observed).
That is, the selection function $S(y_n, z_n)$, which delivers the
probability that source $n$ with these properties would be included in
the catalog, is in general a function of both the observed quantities
$y_n$ and the latent quantities $z_n$.
We will return to this point in \sectionname~\ref{sec:estimate},
where we discuss the determination or estimation of the selection
function.

\section{Basic desiderata for the selection function}\label{sec:desiderata}

There are various important properties that this selection function
does or must or could have.
Here we list and discuss a few of them.

\paragraph{Inputs can be modeled}
The arguments $y_n, z_n$ of the function must be things
that you can predict or compute in your model of the universe.
That is, in what follows, we will be using this selection function to
perform some kind of statistical analysis of the population of
sources, which involves some kind of model with parameters of what the
population contains.
This model has to be able to predict the density of sources in the
$y_n, z_n$ space if this form of the selection function is to be
useful.
Of coure sometimes the selection of sources really does depend on
things you can't easily predict or model!
But the important thing is that any useful approximation to the
selection function---useful for making population inferences---has to
be written in terms of things that you can indeed predict or model in
your population model.
And (as we will discuss below) it is important to make choices with
respect to source selection that make it close to true that the
selection was indeed dependent on things you can model.
This will come back up in \sectionname~\ref{sec:design}.

\paragraph{Things outside your control}
The selection function factorizes into two distinct factors.
One factor is what you might call the \foreign{a priori} selection:
It is what you chose deliberately to put in your catalog.
The other factor is what you might call the \foreign{a posteriori}
selection:
This is what, of the things you chose, that you end up eventually
putting in your catalog, for reasons out of your control.
For example, you might choose to take spectra of all galaxies in some
simple color--magnitude box.
But for some of the galaxies you aren't able, even with good spectra,
to determine redshifts.
Your redshift catalog contains, then, only those galaxies that
\emph{both} were chosen by you \foreign{a priori} and which,
\foreign{a posteriori}, after spectroscopy, were assigned redshifts.
The total selection function is a product of these two selections.

By definition, the \foreign{a posteriori} selection function is out of
your control.
But it is important to do everything you can to make the \foreign{a
  priori} selection function computationally tractable and accurate,
and the \foreign{a posteriori} selection function as pleasant or
minimally disruptive as possible.
In this \documentname\ we will focus on the \foreign{a priori}
selection function, because it is the part under our control.
But the \foreign{a posteriori} selection function is also just as
important, especially when the success rate on targets becomes
substantially less than unity.
In \sectionname~\ref{sec:estimate} we will discuss how to estimate
both of these functions, or their product, no matter how the data
were selected \foreign{a priori}.

\paragraph{Independence}
One assumption we will make here, which is made again two more times
in the likelihood-function justification in
\appendixname~\ref{app:lf}, is an assumption of independence:
We assume that the selection of source $n$ depends only on
the properties $y_n, z_n$ of source $n$.
It turns out that, for pragmatic computational and modeling reasons,
it is very difficult to perform good data analyses if the selection of
one source depends on the properties of other sources.
For one, it destroys the separability of the likelihood function
that we develop in \appendixname~\ref{app:lf}.
For another, it means that your model must be capable of predicting
not just the properties of the sources, but the properties of the
$K$-tuples that are jointly involved in selection.
Most astrophysical models can't do this.

Independence is an assumption at this point, but it will become a contraint
when we consider survey design.
This constraint is easy to accidentally violate when
designing a survey; many surveys do not have this property (because,
for example, the sample is thinned in high-density regions, or only
the brightest sources are taken in some fields).
We will say more about this in \sectionname~\ref{sec:design}.

\paragraph{Select on observables if possible}
One thing that makes life easier, as we discuss in \appendixname~\ref{app:easy},
is if the selection function depends only on the observed
properties $y_n$ and not at all on the latent properties $z_n$.
This makes some of the inference of populations easier.
However, this is only helpful, not a requirement.
The key \emph{requirement} is that the selection function be based
on properties of the source that your model is capable of predicting
or modeling.

Of course the selection of sources, and especially any \foreign{a
  posteriori} selections, can be based on things outside your control.
Therefore it is likely that the selection function $S(y_n, z_n)$ will
not be a completely deterministic function of its arguments $y_n, z_n$.
The probability of selection that $S(\cdot)$ represents is a
probability marginalizing out the unobserved properties of the source
that are affecting its inclusion in the catalog.
It is the existence of such unmodeled effects that makes $S(y_n, z_n)$
a finite probability rather than a deterministic zero or unity.
In the language of causal inference, the selection $S(\cdot)$ is a
probability because there are in general confounders---properties of
the source that are not part of your model that nonetheless have an
effect on the selection.

\paragraph{Counterfactual evaluation}
Finally, one important comment: When we perform population inferences,
we are going to multiply our model by this function $S(y_n, z_n)$ (as
we do explicitly in \appendixname~\ref{app:lf}).
This requires that the selection function be defined for all
conceivable values of the observed data $y_n$ and all conceivable
values of the latents $z_n$.
That is, the selection function will be used to ask
\emph{counterfactual questions} about what would have been observed if
the world had been different.
It is not sufficient to compute the selection function just at the
locations of the observed objects.
If it is only computed at those locations, and used naively (to, say,
upweight the data distribution), the resulting inferences will be 
biased (\citealt{blogpost}).

\section{Survey design for a good selection function}\label{sec:design}

We have looked at what properties a good selection function should
have in \sectionname~\ref{sec:desiderata}, and we have looked at how
it is used in \sectionname~\ref{sec:howto}
and \appendixnames~\ref{app:lf} and \ref{app:easy}.
Now to the main point of this \documentname, which is the question:
How do we design a project or survey such that it will be easy
to obtain a good selection function in the end?
Here we list and discuss a set of considerations.

\paragraph{Don't permit humans into the loop:}
Perhaps the most important consideration is that the selection of sources
must be algorithmic.
This does not mean that the design of the algorithms can't include
human judgement. They must!
This means that once the algorithms are designed and built, they must
be run by a computer; they cannot involve any human judgement.
The fundamental reason for this is that our model of the selection
function cannot (practically) contain within it a model of the human
judgements that were made.
They can, of course, but the selection function model in this case
would necessarily be very approximate.

One strange aspect of this is that the selection of objects can only
rarely make use of catalogs in the historical literature (like the known
globular clusters CITE, the Messier Catalog CITE, and so on).
These historical catalogs (and many contemporary catalogs, for example CITE)
involve human judgement in their creation.
This human judgement is extremely hard to model or account for in any
model of the selection function.
In general, you want to select your sources algorithmically from
catalogs and data that themselves have been generated algorithmically.
If you don't you still might be able to recover the selection function
to some level of precision, but the methods available to you for its
determination (\sectionname~\ref{sec:estimate}) will be limited.
Or you can use the more limited inference methods that don't require
that the selection function be known (\sectionname~\ref{sec:avoid}).

\paragraph{Keep it as simple as possible:}

HOGG SYNTACTICAL

\paragraph{Keep to quantities you can model or predict:}

\paragraph{Avoid making cuts on signal-to-noise:}

\paragraph{Select objects independently:}

In principle the likelihood function involved in inference
(\appendixname~\ref{app:lf}) is a probability for your whole catalog
$\set{x_n}_{n=1}^N$ of sources, given the population-level parameters
$\theta$.
In this context, you don't necessarily need to treat every source
independently; you \emph{could} select collections of objects in
non-trivial ways.
And, at some level, our toy example (\sectionname~\ref{sec:toy}) has
this property, because within any field, we could only take spectra
of 32 sources (HOGG CONFIRM).
That's a selection based on the properties of a set of sources (the
set that fits within one field).

However, in performing inferences on finite computing resources,
it helps enormously, and is (in practice) only really possible, if the 
sources are selected \emph{independently}.
Here ``independently'' means that each source is selected or not based
only on the properties \emph{of that source}.
Really the requirement is that the assumption that they have been
selected independently is a reasonable assumption given the precision
of your inferences.
But the closer you can get to having independent source selection the
better.

There are many operational constraints that a survey can have that
breaks this independence.

HOGG... Too many sources for the available fibers or telemetry.

HOGG... related: Fiber collisions

HOGG... Doing two surveys at once

When the imaging is crowded, so sources are overlapping and must be
deblended before they are photometered and selected, the properties of
a source, as observed, can depend in complex ways on the properties of
the other sources that overlapped it.

\paragraph{Resolve conflicts randomly:}

HOGG When you hit conflict issues, it behooves you to resolve conflicts randomly. Why?

\paragraph{Anticipate spatial conflicts:}

\paragraph{Maintain high a posteriori completeness:}

\section{How do you know that your survey design is good?}\label{sec:verify}

\section{How do you determine the selection function?}\label{sec:estimate}

HOGG SYNTACTICAL

\paragraph{Know thy experiment}
Foo, bar.

\paragraph{Injection testing}
Foo, bar.

\paragraph{Comparison with observed populations}
Foo, bar, but HOGG note that this makes the completeness estimate depend
on the Universe model!

\paragraph{Parameterize it and fit it}
Foo, bar.

\section{How to use the selection function?}\label{sec:howto}

How is the selection function used for inference?
The basic answer is that any model of the universe (the whole galaxy
or the whole Universe or the whole planet population, for examples)
is only a model of what's in your catalog after it has been multiplied
by the selection function.
That is, the catalog model (the model of the catalog-generating process)
is a model of the process that
creates the universe followed by a model of the process that censors or
probabilistically selects sources from that universe for inclusion in the catalog.
The inference of the properties of the universe of sources proceeds
by comparing the catalog model to the observed catalog.
That is, the inference  proceeds by comparing the observed catalog
of sources to a model which is a product of the universe model and the
selection function.

HOGG: HWR SAY: Put an equation in here and refer down to the appendices.

No matter how the comparison is going to be done (the comparison between
observed catalog and catalog model), it is nonetheless
useful to think in
terms of a \emph{likelihood function}, or probability for the data given
the parameters $\theta$ of interest.
The likelihood function is the critical tool for Bayesian inference, and
it is also the critical tool for constructing frequentist estimators that
saturate information-theoretic bounds.
It also follows that if the investigator \emph{can} write a likelihood
function, then the investigator does know enough about the data to perform
any kind of information-saturating inference.
Indeed, our desiderata for the selection function given in
\sectionname~\ref{sec:desiderata} all flow from considerations related
to this likelihood function.

The message is that statistical legacy value is maximized for a
statistical project when it is possible for investigators to construct
a justifiable and computationally tractable likelihood function.
Because the details are somewhat off-topic, we use the
\appendixnames\ to construct a version of this likelihood function.
The general
answer is given in \equationname~(\ref{eq:lf}) and a simpler version
is presented in \appendixname~\ref{app:easy}.
These likelihood functions are possible to construct and compute when the
selection-function has the good properties we argue for in
\sectionname s~\ref{sec:desiderata} and \ref{sec:design}.

\paragraph{Should you multiply the model or divide the data?}
We said that the model for the catalog is the universe model times a
selection function.
But doesn't that tempt us (instead) to divide the data by the selection
function---or really re-weight the data by the inverse of the slection
function---to get a weighted data set that looks much more like the
universe than the unweighted data set?
That's a sensible idea, and the basis of a lot of astrophysics, going
all the way back to the 1/V-max method for the determination of the quasar
luminosity function (\citealt{schmidt}) and continuing in a long literature on the
luminosity functions of galaxies (for example, \citealt{cowie, blanton, faber}).
However, it is not the best use of the data:
The approach of re-weighting the data by the inverse of the
selection function tends to be both high-variance and biased.

To see why this inverse-selection weighting is high-variance, think about
the sources that were detected in the low-selection-probability parts of
the survey.
THese sources will be given very large weights.
That is, a lot of the effective mass in the data can become dominated by
a small number of sources.
And, worse than that, these sources will tend to be near the edge of
what's detectable or at the edges of the effective parameter space, so
they are likely to be less reliable in general, or possibly even outliers.
The disparity in the weights leads to a reduction in the effective size
of your catalog:
If you have $N$ sources with weights $w_n$, and different catalog entries get
different weights, the effective size $N_\mathrm{eff}$ of your catalog
is no longer $N$.
It is given by
\begin{equation}
N_\mathrm{eff} = \frac{\left[\sum_{n=1}^N w_n\right]^2}{\sum_{n=1}^N w_n^2} < N \quad .
\end{equation}
This effective number gets smaller as the disparity in the weights increases.

To see why this inverse-selection weighting is usually biased,
consider the case in which the goal is to visualize the population as
a weighted number of objects (or sum of weights) in bins in the $y$
space.
If the inverse selection probabilities have been correctly estimated
and applied, you might think that this will be a (possibly noisy)
unbiased estimator of the true population.
After all, you have corrected for selection!
However, consider a histogram bin in which the selection probability
varies substantially \emph{within} the bin.
This will often happen at the edges of the selected region in the $y$
space.
In this bin, the sources you see are not fairly distributed in the
bin: You tend to find your sources in that bin at the edge of the bin
that has higher completeness.
Thus the mean of the inverse selection probability over selected
sources is not equal to the mean of the inverse selection probability
over the whole bin.
And indeed, neither of these means is precisely what one wants to
compare to the un-censored universe model (this point is made clearly
by \citealt{blogpost}).

Thus the best way to perform population-level inferences is not to
re-weight the data by the inverse of the selection function.  It is to
weight the universe model by the selection function, and make
quantitative comparisons between the weighted universe and the
unweighted data.
However, it should be noted that when you weight the data by the
inverse of the selection function, you only need to know the value of
the selection function \emph{at each actual datum}.
This means that you don't need to know it everywhere.
And sometimes, for reasons outside your control, you \emph{do} only
know the selection function at the locations of the data.
It's not an argument \emph{for} weighting by inverse selection, but it
is some kind of pragmatic justification for this method of last
resort.

\section{Can you avoid using the selection function entirely?}\label{sec:avoid}

HOGG Oddly the answer to this is \emph{yes}. Conditional probabilities!

HOGG This costs information. Tons of information. Example: Go back to the toy.

\section{Contamination and backgrounds}\label{sec:bg}

HWR: DO WE HAVE A SECTION ON CONTAMINATION? I feel like we need to,
but if we do, it brings in many new concepts and issues.

\section{How well do you need to know the selection function?}\label{sec:quality}

Once you have a LF you can start saying things about information.

There is almost certainly a result (a la Bovy) that we don't need to know
things about the SF that don't project strongly onto our parameters of interest.
Let's make that explicit here, cite Bovy.

When we say you don't have to know everything about the SF, that could be
in space, in time, in Fourier Space, in magnitude, and so on.

\section{Open issues}

\paragraph{Performing multiple surveys at once}
Note that if you are doing both MWM and BHM at the same time, there will be
spatial conflicts.

That leads to the selection of one star being affected by properties of the
BHM targets. Can we deal with that? I think yes in this case.

But now think about one MWM target category conflicting with another MWM target
category. That might lead to ``confounder'' situations where the relevant selection
information can't be modeled in any model.

Idea of pre-down-sampling by a fixed, chosen function before starting, to avoid
all spatial conflicts. This has interesting properties. It is a good idea; it isn't
\emph{perfectly} safe (why?), but it is pretty good.

\paragraph{Adaptive strategies}
What changes for, or how do we think about, strategies in which you want to
adaptively deepen certain interesting targets, but cut off uninteresting targets,
while you are operating.

There is a one-target version of this (this target is low metallicity, deepen it).

And there is a target-category version of this (we find that we can't measure WDs
accurately enough to do our science so we drop the category). Or we change the exposure
times we use \emph{during} the survey, based on what we see.

Are these both interesting to our study?

\paragraph{Select on apparent or intrinsic properties?}
HOGG A great example is dust extinction and reddening.

\paragraph{Likelihood-free and simulation-based inferences}
HOGG Note that, in principle, the survey only needs to be simulated, it doesn't have
to be understood!

\paragraph{Do we need to model the humans?}

HOGG Even if you have done everything right, you made your choices because of things
you know, observationally, about the sky and the sources of interest.

HOGG This means that even your \foreign{a priori} selection isn't really prior to
your inferences about the sources you are studying.

HOGG Does this lead to biases? Almost certainly.

HOGG Call out the failure-to-reproduce issue.

\section{Discussion}

Come back to the assumptions made and talk about what happens when we
adjust or weaken or don't obey these.

For example, what of this discussion carries over to two-point functions?

\acknowledgements We thank the \project{Milky Way
  Mapper} Collaboration (part of the \project{SDSS-V} Collaboration)
for help with refining these principles.
This work benefitted from our conversations with
  Will Farr (Stony Brook),
  Dan Foreman-Mackey (Flatiron),
  and
  Jan-Torge Schindler (MPIA).

\begin{thebibliography}{}
\bibitem[Bernstein \etal(2004)]{bernstein} Bernstein, G.~M., Trilling, D.~E., Allen, R.~L., \etal, 2004, \aj, 128, 1364
\bibitem[Blanton \etal(2001)]{blanton} Blanton, M.~R., Dalcanton, J., Eisenstein, D., \etal, 2001, \aj, 121, 2358
\bibitem[Bovy \etal(2012)]{bovy} Bovy, J., Rix, H.-W., Liu, C., \etal, 2012, \apj, 753, 148
\bibitem[Colless \etal(2001)]{2df} Colless, M., Dalton, G., Maddox, S., \etal, 2001, \mnras, 328, 1039
\bibitem[Cowie \etal(1996)]{cowie} Cowie, L.~L., Songaila, A., Hu, E.~M., \etal, 1996, \aj, 112, 839
\bibitem[Faber \etal(2007)]{faber} Faber, S.~M., Willmer, C.~N.~A., Wolf, C., \etal, 2007, \apj, 665, 265
\bibitem[Foreman-Mackey(2014)]{blogpost} Foreman-Mackey, D., 2014, The Histogram (blog post), \url{https://dfm.io/posts/histogram1/}
\bibitem[Foreman-Mackey \etal(2014)]{exopop} Foreman-Mackey, D., Hogg, D.~W., \& Morton, T.~D., 2014, \apj, 795, 64
\bibitem[Kollmeier \etal(2017)]{sdssv} Kollmeier, J.~A., Zasowski, G., Rix, H.-W., \etal, 2017, arXiv e-prints, arXiv:1711.03234
\bibitem[Loredo \& Wasserman(1995)]{loredogrb} Loredo, T.~J., \& Wasserman, I.~M., 1995, \apjs, 96, 261 
\bibitem[Loredo(2004)]{loredo} Loredo, T.~J., 2004, American Institute of Physics Conference Series, 195
\bibitem[Mandel \etal(2019)]{mfg} Mandel, I., Farr, W.~M., \& Gair, J.~R., 2019, \mnras, 486, 1086
\bibitem[Marshall \etal(1983)]{marshall} Marshall,~H.~L., Avni,~Y., Tananbaum,~H., \etal, 1983, \apj, 269, 35
\bibitem[Schmidt(1968)]{schmidt} Schmidt, M.\ 1968, \apj, 151, 393
\bibitem[York \etal(2000)]{sdss} York, D.~G., Adelman, J., Anderson, J.~E., \etal, 2000, \aj, 120, 1579
\end{thebibliography}

\appendix

\section{Dimensions of probabilistic quantities}

The development of the catalog likelihood function in
\appendixname~\ref{app:lf} depends on probabilistic quantities that
obey particular rules.

A probability density function (pdf) $p(\cdot)$ has the properties
\begin{gather}
0 \leq   p(a\given b)        \quad \mbox{for all $a, b$,} \\
1 = \int p(a\given b)\,\dd a \quad \mbox{for all $b$,}
\end{gather}
where the integral is over the entire conceivable domain of $a$.

A rate density or density function $\lambda(\cdot)$ has the properties
\begin{gather}
                0 \leq   \lambda(a\given b)        \quad \mbox{for all $a, b$,} \\
\left<N(b)\right> = \int \lambda(a\given b)\,\dd a \quad,
\end{gather}
where $\left<N(b)\right>$ is an expected number
(which will depend, in general, on $b$). Once again, the integral
is over the entire conceivable domain of $a$.

A selection function $S(\cdot)$ has the property
\begin{equation}
0\leq S(a, b)\leq 1 \quad \mbox{for all $a, b$.}
\end{equation}
The selection function becomes \emph{deterministic} when it is
everywhere either zero or unity.

\section{The catalog likelihood function}\label{app:lf}

As we note in \sectionname~\ref{sec:howto}, the way to perform a populations
inference is to multiply your model of the uncensored universe by the
selection function and compare it to the data.
The best comparison is made through a justified likelihood function.
Here we construct that function.
This discussion is worth comparing to prior discussions (including
especially \citealt{marshall, loredo, bovy, exopop, mfg}).

As we posited in the main text, we are designing a catalog or a targeted
spectroscopic project
with population-level statistical goals, for example HOGG XXX or YYY.
Imagine that the model of the population (for example the model
of XXX or YYY) has parameters $\theta$.
These parameters $\theta$ describe the properties of the true distribution
of astronomical objects (galaxies, stars, supernovae, planets, or whatever)
that would be oberved if we could see everything of interest completely and
with high fidelity.
The object (by assumption) is to determine these population-level parameters
$\theta$.

The likelihood function factorizes into three factors:
The first factor is the population model.
We assume that the population-level parameters $\theta$ generate the
sources of interst.
That is, the parameters $\theta$ parameterize a probability density
function (pdf) $p_0(\set{z_n}\given\theta)$
for the set of true properties $\set{z_n}$ of all of the sources $n$
in the Galaxy (we will posit for now that the objects of study are
stars in the Milky Way; they could be galaxies in the Universe or
planets in the solar neighborhood or anything else).
This is the first of the three factors that make up the likelihood
function.
We subscript it with a $0$ to indicate that
this is a model of the true world;
it is the pdf you would observe if you saw everything perfectly.

This population model $p_0(\set{z_n}\given\theta)$ could have any arbitrarily
complicated form.
However, for our purposes here, we are going to assume it has a certain kind
of separability.
That is, we are going to assume that the objects are conditionally independent,
or independently drawn, conditioned on the parameters $\theta$.
This is like saying that the model is a variable-rate Poisson process.
Under this assumption, the model can be written as a product over sources (or sum in the log), like so:
\begin{equation}
\ln p_0(\set{z_n}\given\theta) = \sum_n \ln \lambda_0(z_n\given\theta) - Q(\theta)
\end{equation}
where $Q(\theta)$ is a normalization factor that we will discuss below,
and the
individual-source models $\lambda_0(z_n\given\theta)$ represent the
density functions (number of sources per unit volume in the $z$-space)
for individual sources given the population parameters $\theta$.
This assumption---that the population model can be written as a product over
sources---is used frequently in astrophysics projects.
In this approximation, there can be no information two-point or
higher-order statistics of the data; or nothing can be learned from
source pairs that can't be learned combining information from the
individual members of those pairs.
Among other things, this formalism or assumption is at odds with
models that specify source--source correlation functions.

The second factor in the likelihood function is
a noise model $p(y_n\given z_n)$, which gives a probability density for
observing observed properties $y_n$ for source $n$ under the assumption
that it has true properties $z_n$.
In the case that the true properties are things like true brightnesses
and true colors for the stars, this noise model $p(y_n\given z_n)$ is
very simple (it is just the noise properties of the data.
This noise model is more complex if the true properties are things like
stellar masses or ages, where the connection to the noisy observables is
less direct.
This noise model depends in detail on having a good model for how the
observing instrumentation works, or in fact how the whole chain of causation
works from the true properties $z_n$ to the observed values $y_n$.
If this model contains uncertain parameters (unknown gains, say, or
unknown read noise, or \foreign{etc.}), then this function
$p(y_n\given z_n)$ could be conditioned on additional noise-model
parameters and these could be fit or inferred or marginalized out
simultaneously with the population-level parameters $\theta$.
We discuss this case in the main text in \sectionname~\ref{sec:estimate}.

It is worth emphasizing an assumption here:
By writing this noise model at the individual-source level as
$p(y_n\given z_n)$
and not at the whole-catalog level
$p(\set{y_n}\given\set{z_n})$,
we have assumed that the observed properties $y_n$ of source $n$
depend only on the true properties $z_n$ \emph{of that source}.
This is another kind of assumption of independence:
Technically this independence can be broken in real surveys by,
for example, calibration noise, which correlates the wrongness of
every source in some patch.
But we make this assumption because it is often very close to being
correct, and it will keep the likelihood function tractable.

The third factor in the likelihood function is the selection function,
described in the previous \sectionname.
This is the function $S(y_n,z_n)$ that gives the probability that a
source was included in the catalog, given its observed or true
properties (or both).
As with the noise model, the selection function can also depend on
unknown parameters, which can also be fit or inferred or marginalized
simultaneously with the population-level parameters $\theta$.
We will return to this point HOGG WHERE?
Once again, in this third factor we have made a third assumption of
independence, which we discussed in \sectionname~\ref{sec:desiderata}.

The three likelihood factors combine into the complete, marginalized
likelihood function $p(\set{y_n}\given\theta)$ as follows:
\begin{gather}
\ln p(\set{y_n}_{n=1}^{N}\given\theta)
 = \sum_{n=1}^N \ln\int\lambda_0(z_n\given \theta)\,p(y_n\given z_n)\,S(y_n, z_n)\,\dd z_n - Q(\theta)
\label{eq:lf}
\\
Q(\theta) = \int\!\int\lambda_0(z_n\given \theta)\,p(y_n\given z_n)\,S(y_n, z_n)\,\dd z_n\,\dd y_n
\quad .
\end{gather}
This likelihood function is \emph{marginalized} in the sense that we
have integrated out the true latent properties $\set{z_n}_{n=1}^N$.

This (\ref{eq:lf}) is the likelihood function used in our exoplanet
populations work (\citealt{exopop}).
It is very related to likelihoods
used in other context by us and others (\citealt{marshall, loredogrb, bernstein,
  bovy, mfg}).
There are some differences however;
in many projects, the selection function
does not appear in the sum; it only appears in the normalization $Q(\theta)$.
This difference flows from some different assumptions about how the
data are generated and censored; in particular these other projects assume
that the censoring depends only on the observed data $y_n$.
We are (effectively) assuming that the universe plus the survey are
jointly working together to generate a data stream which might be
censored on the basis of both observed data $y_n$ and latent source
properties $z_n$, and where the censoring might be probabilistic, even
conditioned on both $y_n$ and $z_n$.
That jointly generated data stream is being treated (by us) as a
variable-rate Poisson process.
Further discussion of the subtleties of the derivation of this
likelihood is out of scope in this \documentname.

But importantly, these subtlties do not matter for the subject at hand:
The comments we make in the main text about the form of the
selection function, and about requirements on survey design and target
selection, are all invariant to this subtle statistical question about
the form of the likelihood function.

\section{Catalog likelihoods: The easy cases}\label{app:easy}

Continuing with the form of the likelihood function, here we point out
situations in which it becomes simpler than the expression presented
in equation~(\ref{eq:lf}).
The first observation to make is that the integrals inside the sum are
over the true properties $z_n$, not the observed properties $y_n$.
This means that if the selection function $S(\cdot)$ depends only on
the observed properties $y_n$ and not on any unobserved latent parameters,
the selection function $S(y_n)$ can be put in front of that integral.
And since the logarithm is being taken, and $S(y_n)$ (by assumption)
doesn't depend on the population-level parameters, it can be removed
entirely from the expression as an irrelevant constant term (this is
noted previously in \citealt{bovy}).
In this case, the selection function appears in the integral
$Q(\theta)$ but not at all in the sum of logarithms, which
makes it effectively identical to the likelihood
constructed under the alternate assumptions mentioned above
(\citealt{loredo, mfg}), where, indeed, the censoring is assumed to
depend only on the observed data $y_n$.

In many cases, it is not an investigator choice whether the selection
function is delivered in terms of $y_n$ or $z_n$ or both, but when it
is a choice, it is useful to choose to deliver it in terms of the
observed quantities $y_n$.
We will return to this point in \sectionname~\ref{sec:estimate}, where
we discuss how the selection function is estimated or determined.

The other---and perhaps more general---observation to make is that
sometimes the source selection is being performed in \emph{extremely
  good data}.
For example, when targets are being chosen for spectroscopy, in
imaging that is much deeper than the magnitude limits for the
spectroscopy.
This relates to our motivating examples of \project{SDSS} and
\project{SDSS-V}, where the spectrographs are being used on sources 5
to 10 magnitudes brighter than the limits of the imaging.
In this case, it can become (effectively) the case that there is no
distinction to be made between the data $y_n$ on source $n$ and the
true, latent properties $z_n$ of source $n$.
That is, at the precision of interest, it can become the case that
$y_n\equiv z_n$, or the observed data are effectively the source's
true properties.
In this case the noise model $p(y_n\given z_n)$ becomes a
delta function and the likelihood function becomes
\begin{gather}
\ln p(\set{y_n}_{n=1}^{N}\given\theta)
 = \sum_{n=1}^N \ln\lambda_0(y_n\given \theta) - Q(\theta) + \mbox{const}
\label{eq:good}
\\
Q(\theta) = \int\lambda_0(y_n\given \theta)\,S(y_n)\,\dd y_n
\quad ,
\end{gather}
where the $S(y_n)$ has disappeared from the sum because it (or really
the log of it) is an irrelevant constant (independent of the important
parameters $\theta$).
This form has appeared and been used for inference previously (\citealt{bernstein, bovy}).
That irrelevant constant can't be ignored if the selection function
itself has free parameters that you want to fit or infer or
marginalize simultaneously with your populations inference;
we discuss this case in the main text in \sectionname~\ref{sec:estimate}.

Of course, in this ``good-data'' limit, there is still an
\foreign{a posteriori} selection function acting---sources can be
observed but not successfully analyzed and included in the
catalog---but again in these projects, often the spectroscopy is taken
at such high signal-to-noise that \foreign{a posteriori}
incompleteness is not a significant issue.
If we add that criterion to the criteria for being in the good-data
limit, the simpler form of the likelihood function in
\equationname~(\ref{eq:good}) can be justified.
That is, the good-data limit can be achieved, and has been for many
high signal-to-noise surveys (HOGG CITE THINGS?), but the criteria
are definitely severe.

\end{document}
