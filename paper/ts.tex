% This file is part of the TargetSelection project
% Copyright 2019 the authors.

% To-dos
% ------
% - make a to-do list
% - make style notes
% - should we have an ``open issues'' section?
% - do we say what ``counterfactual'' means in this context?
% - separate high snr from low snr selections?
% - make sure to cite exopop, loredogrb, loredo, bovy, and blogpost.
% - ...

% Style notes
% - 

\documentclass[modern]{aastex62}
% \usepackage{graphicx, xcolor}
% \usepackage[sort&compress]{natbib}
\usepackage{amsmath}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\pc}{\unit{pc}}
\newcommand{\kpc}{\unit{kpc}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\given}{\,|\,}
\newcommand{\set}[1]{\left\{{#1}\right\}}

% chemical macros
\newcommand{\abundance}[2]{\mathrm{\left[{#1}/{#2}\right]}}
\newcommand{\alphafe}{\abundance{\alpha}{Fe}}

% text macros
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et al.}}

% margins and headers and spacing changes
\renewcommand{\twocolumngrid}{}  % trust in Hogg -- bibliography fix
\setlength{\parindent}{1.4em}    % trust in Hogg -- make indents look like squares
\addtolength{\topmargin}{-0.4in} % trust in Hogg -- taller page
\addtolength{\textheight}{0.8in} % trust in Hogg -- taller page
\shorttitle{requirements for source selection}
\shortauthors{hogg \& rix}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
% \graphicspath{ {figures/} }
%\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{\textbf{%
Source selection for catalogs and spectroscopic follow-up:\\
General requirements for statistical and legacy value%
}}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}
\affil{Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}

\author[0000-0003-4996-9069]{Hans-Walter Rix}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Many new large cataloging and mapping projects are being built and operated
in the areas of
large-scale structure, Milky Way cartography, stellar physics,
exoplanet discovery, and explosive transients.
These must select sources from imaging (and other) data for inclusion
in the catalog or study, or for spectroscopic follow-up observation.
These projects generally want their results to be useful for
statistics, population inferences, and legacy use in future projects.
% aims
Here we lay out some general principles for source selection that will
create this legacy value.
% methods
The key idea, from which all principles flow, is that the survey must
be able to produce a computationally tractable
likelihood function for parameters of interest.
This, in turn, recommends that selection be performed such that it is possible
to construct a relatively simple selection function, or a probability
with which any source (real or counter-factual) would have been selected
in a repeat of the survey.
% results
Among our findings and recommendations are the following:
You want to base your selection as much as possible on quantities that
can be reliably and straightforwardly predicted by the relevant
physical models.
You want to avoid having the selection of one source
depend strongly on the properties of \emph{other} sources.
You don't need to know everything about how sources were selected, you
just need to know those aspects of selection that project most strongly
onto the matters of great interest.
If you are presented with survey data that were selected badly,
it is still possible to make a conditional likelihood function that can be
used for inference, but at information-theoretic cost.
\end{abstract}

\keywords{\raggedright
 catalogs
 ---
 methods:~observational
 ---
 methods:~statistical
 ---
 surveys
 ---
 techniques:~spectroscopic
 ---
 telescopes
}

\section{Why source selection matters}

Many projects in conteporary astrophysics proceed by obtaining
wide-field imaging and then selecting targets for spectroscopic
follow-up.
This mode is very old, but it became truly data-intensive with
the \project{Sloan Digital Sky Survey} (\citealt{sdss})
and \project{2dF} (\citealt{2df})
projects, which mapped the large-scale structure to test and learn the
cosmological model.
Now there are projects with this character in a range of different
astrophysics sub-fields, including cosmology, Galactic archaeology,
stellar astrophysics, transient characterization, and exoplanet search.
The successes have been legion:
We know the parameters of the cosmological model at percent-level
precision, supernovae of many types have been found and monitored,
hundreds of exoplanets have been spectroscopically confirmed,
and we have three-dimensional maps of the Milky Way in
element abundances and kinematics.
In this \documentname, we would like to emphasize the point that the
quality of the results from these surveys depends both on the
spectroscopic data they take, and equally (or even more importantly)
on the way the spectroscopic targets were selected from the original
imaging data.

More generally, whenever a set of astronomical sources has been chosen
for study... assembled into a catalog... HOGG

Once a project is building a catalog with thousands to millions of entries, the
end-to-end uncertainties in the results it produces tend to be
dominated not by the shot noise (square-root-of-$N$) or sample size,
but rather the ability to understand the selection effects.
That is, to make precise measurements of a population of galaxies or
stars or planets, we need to know what galaxies or stars or planets
were and were not selected, and what would
have been selected differently if the population of interest had been
different.
That is, we need to understand the \emph{selection function} of the
survey.
We will say more about this below, but the selection function is,
loosely, the probability that a source on the sky would have been
included, conditioned on the
position and physical or observational properties of that source.
The object of this \documentname\ is to elucidate the aspects of
source selection are most relevant to understanding and obtaining a
computationally tractable model of the selection function.

Even when a project is beautifully designed and
executed, it isn't always possible to understand or model this
selection function accurately.
For example, when sources are selected by hand or by
a human process or including a human classification or vetting, it is often difficult to
figure out what would or would not have been included in different
(counter-factual) situations.
For another, when each source is chosen in a way that depends strongly
on properties of that source that are difficult to understand within
the context of the physical model, it is difficult to build a useful
model of the selection function.
It might be accurate, but not useful!
For another, when one source's selection depends on the properties
of other sources in the vicinity of that source, the selection
function might depend on spatial structures that are hard to model or
hard to manipulate computationally.
For yet another, when a survey is beautifully selected \foreign{a priori},
but has low
\foreign{a posteriori} success rate on follow-up observations that
are required for inclusion in the catalog, the selection
function might depend on properties of the sources that are hard to
understand or model.

In this \documentname, we develop guidelines for source selection and
catalog generation
that, if followed, will improve the chances that it will be possible
to build a simple and accurate selection function model.
This, in turn, will improve the chances that the resulting data
will be useful for building a precise model of the relevant source
population.
This, in turn, increases the value of the project for its investigators,
and the legacy value for the community.

In what follows, we make some assumptions about what's important,
scientifically, about the project of interest, and what is not.
We will assume that the project wants to understand statistical properties
of the objects under study.
For example, the mean density of galaxies, or the fraction of galaxies
of different types in different environments;
or the relative rates of different types of supernovae;
or the density of stars as a function of chemical abundances and radius in the Milky Way disk;
or the rate at which stars host planets in a certain mass and period range.
But we will also assume that the project cares about these goals more,
or equally with, discovery rate or sample purity.
That is, the rules that we elucidate will put (at least slightly)
legacy statistical value in competition with discovery rate or sample
purity.
This is because we will put restrictions on what can be used for
selection, and those restrictions will come at the cost of some
opportunities to improve purity.
We will also assume (in this \documentname) that the survey is performing
its sample selection in a single pass, not adaptively, using some
targetted observations, using these to refine the target sample, and
iterating observationally.
This makes our discussion less useful to those spectroscopically following up exoplanet
candidates and supernovae, the characterization of which evolves with
the number of spectroscopic epochs.
It is possible to generalize what's presented here to this time-dependent, adaptive
case, but it is out of scope for this \documentname.
Finally, we will not address special issues that come in to play in the
measurement of 2-point functions (like correlation functions and power
spectra) or higher-order spatial statistics.
The measurement of these spatial statistics bring additional issues
that are also out of scope here.

\textbf{HWR: Should we explain some example scientific goals here, and make
them ``fiducials'' for what follows?}

\section{The selection function}

Our goal is to select our sources such that it is possible to
construct a computationally tractable and relatively accurate model of
the selection function, or the probability that a source would have
selected for the catalog, as a function of that source's properties.
The reason that this is our goal---the reason that we want a selection
function---is that we need it to perform population-level inferences.
More technically, we need it to construct a likelihood function, as we
will argue in the next \sectionname, and we need that likelihood
function in order to make information-theory-optimal inferences.
But even if we don't want to be information-theory optimal: The
selection function is the ratio of what is observed to the whole
population, so any inference about or reconstruction of the whole
population will require it.

In what follows, every source $n$ we could observe has two kinds of
properties.  One is its true properties $z_n$, which are latent
properties (technically unobservable), and the other are its observed
properties $y_n$, which are the instrument readings or outputs on that
source, if such outputs are or were made.
In general $y_n$ and $z_n$ as vectors or blobs of properties.
For concreteness, you could think of the $z_n$ as being the true
temperature, surface gravity, composition, and position (in phase
space) of a star, and the $x_n$ as the measured brightness of the star
in various passbands, plus maybe measured parallax and proper motion.
Or, even simpler, the $z_n$ could be the true brightnesses that would
have been measured with much better measurements (perfect data), and
the $y_n$ are the actual measurements we got.

In principle, the appearance of the source in our catalog can depend
on either or both of these things.
That is, the selection function $S(y_n, z_n)$, which delivers the
probability that a source with these properties would be included in
the catalog, is in general a function of both the observed quantities
$y_n$ and the latent quantities $z_n$.

HOGG SYNTACTICAL REFERENCE

\section{The likelihood function}

As we posited above, we are designing a catalog or a targeted
spectroscopic project
with population-level statistical goals, for example HOGG XXX or YYY.
Here we will imagine that the model of the population (for example the model
of XXX or YYY) has parameters $\theta$.
These parameters $\theta$ describe the properties of the true distribution
of astronomical objects (galaxies, stars, supernovae, planets, or whatever)
that would be oberved if we could see everything of interest completely and
with high fidelity.
The object (by assumption) is to determine these population-level parameters
$\theta$.

No matter how the data analysis is going to proceed, it is nonetheless
useful to think in
terms of a \emph{likelihood function}, or probability for the data given
the parameters $\theta$ of interest.
The likelihood function is the critical tool for Bayesian inference, and
it is also the critical tool for constructing frequentist estimators that
saturate information-theoretic bounds.
It also follows that if the investigator \emph{can} write a likelihood
function, then the investigator does know enough about the data to perform
any kind of information-saturating inference.
Therefore, we maximize statistical legacy value for a statistical project
when we make it possible for investigators to construct a justifiable and
computationally tractable likelihood function.

For our purposes here, the likelihood function factorizes into three
factors:
The first factor is the population model.
We assume that the population-level parameters $\theta$ generate the
sources of interst.
That is, the parameters $\theta$ parameterize a probability density
function (pdf) $p_0(\set{z_n}\given\theta)$
for the true properties $\set{z_n}$ of all of the sources $n$
in the Galaxy (we will posit for now that the objects of study are
stars in the Milky Way; they could be galaxies in the Universe or
planets in the solar neighborhood or anything else).
This is the first of the three factors that make up the likelihood
function.
We subscript it with a $0$ to indicate that
this is a model of the true world;
it is the pdf you would observe if you saw everything perfectly.

This population model $p_0(\set{z_n}\given\theta)$ could have any arbitrarily
complicated form.
However, for our purposes here, we are going to assume it has a certain kind
of separability.
That is, we are going to assume that the objects are conditionally independent,
or independently drawn, conditioned on the parameters $\theta$.
This is like saying that the model is a variable-rate Poisson process.
Under this assumption, the model can be written as a product over sources, like so:
\begin{align}
    p_0(\set{z_n}\given\theta) &= \exp -Q(\theta)\,\prod_n p_0(z_n\given\theta) \\
\ln p_0(\set{z_n}\given\theta) &= \sum_n \ln p_0(z_n\given\theta) - Q(\theta)
\end{align}
where $Q(\theta)$ is a normalization factor that we will discuss below
(after we have introduced the selection function), and the individual-source
models $p_0(z_n\given\theta)$ represent the pdfs for individual
sources given the population parameters $\theta$.
This assumption---that the population model can be written as a product over
sources---is used frequently in astrophysics projects.
In this approximation, there can be no information two-point or
higher-order statistics of the data; or nothing can be learned from
source pairs that can't be learned combining information from the
individual members of those pairs.
Among other things, this formalism or assumption is at odds with
models that specify source--source correlation functions.

The second factor in the likelihood function is
a noise model $p(y_n\given z_n)$, which gives a probability density for
observing observed properties $y_n$ for source $n$ under the assumption
that it has true properties $z_n$.
In the case that the true properties are things like true brightnesses
and true colors for the stars, this noise model $p(y_n\given z_n)$ is
very simple (it is just the noise properties of the data.
This noise model is more complex if the true properties are things like
stellar masses or ages, where the connection to the noisy observables is
less direct.
This noise model depends in detail on having a good model for how the
observing instrumentation works, or in fact how the whole chain of causation
works from the true properties $z_n$ to the observed values $y_n$.
If this model contains uncertain parameters (unknown gains, say, or
unknown read noise, or \foreign{etc.}), then this function
$p(y_n\given z_n)$ could be conditioned on additional noise-model
parameters and these could be fit or inferred or marginalized out
simultaneously with the population-level parameters $\theta$. WE MAY
RETURN TO THIS LATER.

It is worth emphasizing an assumption here:
By writing this noise model at the individual-source level as
$p(y_n\given z_n)$
and not at the whole-catalog level
$p(\set{y_n}\given\set{z_n})$,
we have assumed that the observed properties $y_n$ of source $n$
depend only on the true properties $z_n$ \emph{of that source}.
This is another kind of assumption of independence:
Technically this independence can be broken in real surveys by,
for example, calibration noise, which correlates the wrongness of
every source in some patch.
But we make this assumption because it is often very close to being
correct, and it keeps the likelihood function (below) tractable.

The third factor in the likelihood function is the selection function,
described in the previous \sectionname.
This is the function $S(y_n,z_n)$ that gives the probability that a source was included in the
catalog, given its observed or true properties (or both).

HOGG: Note that $p(a\given b)$ is a pdf for $a$ given $b$; it obeys
$\int p(a\given b)\,\dd a = 1$ and $S(a,b)$ is a pure probability; it
obeys $0<S(a, b)<1$.

HOGG: Note the independence assumption AGAIN. It came in three times, once
for each factor.

These three factors combine into the complete, marginalized likelihood
function $p(\set{y_n}\given\theta)$ as follows:
\begin{align}
p(\set{y_n, z_n}\given\theta) &= foo
\\
p(\set{y_n}\given\theta) &= \int\!\int\!\int\!\cdots\int p(\set{y_n, z_n}\given\theta)\,\prod_n\dd z_n
\end{align}

HOGG: SYNTACTICAL REFERENCE POINT

Define the Poisson likelihood, including the overal normalization, which matters here.

There is an \foreign{a priori} selection function and an \foreign{a posteriori}
selection function. You can only control one of these fully, unless you have near-unit
completeness on your spectral analysis.

Here might be the place to note that the selection function has to something of
which we can ask \emph{counterfactual} questions. It can't just be defined at
the locations of the data.

Here we should also bring up the idea of confounders. If you think the SF
is S(x) but it is actually S(x, w) and you don't track or can't predict w,
then you can't write an accurate LF.

%% %% DFM writes (2019-08-12):
%% I don't know of any other good examples, but (this might be beyond the
%% scope, but) I'd be very interested to hear what you guys come up with
%% when trying to combine these ideas with uncertainty propagation. Tom
%% Loredo and Will Farr both have lots of ideas about this (but not much
%% that has actually been published as far as I can tell) and it's pretty
%% subtle! The root question is - does the selection function just enter
%% in the exponent or also in the product (y'know). I think that it could
%% enter both places depending on your model assumptions, but I'm not
%% sure.

\section{Correct the data or correct the model?}

This section should include the point that the tradition is to divide out
the selection function. That's not good, for divide-by-zero or variance
reasons. There are words in a DFM paper relevant to this.

The divide-by-selection-function methods have the great property that you don't need
to know the SF everywhere. You only need to know it at your data. But then
it can't penalize models that put high weight on locations with near-zero
completeness. That is, it is biased in these regions and wrong.

\section{The easy case: Selection with excellent data}

\section{Contamination and backgrounds}

\section{Information theory}

Once you have a LF you can start saying things about information.

There is almost certainly a result (a la Bovy) that we don't need to know
things about the SF that don't project strongly onto our parameters of interest.
Let's make that explicit here, cite Bovy.

When we say you don't have to know everything about the SF, that could be
in space, in time, in Fourier Space, in magnitude, and so on.

\section{Spatial selection functions}

Select on clustering? What's wrong with that?

\section{Doing multiple surveys at once}

Note that if you are doing both MWM and BHM at the same time, there will be
spatial conflicts.

That leads to the selection of one star being affected by properties of the
BHM targets. Can we deal with that? I think yes in this case.

But now think about one MWM target category conflicting with another MWM target
category. That might lead to ``confounder'' situations where the relevant selection
information can't be modeled in any model.

Idea of pre-down-sampling by a fixed, chosen function before starting, to avoid
all spatial conflicts. This has interesting properties. It is a good idea; it isn't
\emph{perfectly} safe (why?), but it is pretty good.

\section{Adaptive strategies}

What changes for, or how do we think about, strategies in which you want to
adaptively deepen certain interesting targets, but cut off uninteresting targets,
while you are operating.

There is a one-target version of this (this target is low metallicity, deepen it).

And there is a target-category version of this (we find that we can't measure WDs
accurately enough to do our science so we drop the category). Or we change the exposure
times we use \emph{during} the survey, based on what we see.

Are these both interesting to our study?

\section{Discussion}

Come back to the assumptions made and talk about what happens when we
adjust or weaken or don't obey these.

For example, what of this discussion carries over to two-point functions?

\acknowledgements We thank the \project{Milky Way
  Mapper} Collaboration (part of the \project{SDSS-V} Collaboration)
for help with refining these principles.
This work benefitted from our conversations with
  Dan Foreman-Mackey (Flatiron).

\facilities{
ESA \project{Gaia},
}

\software{
foo,
bar
}

\begin{thebibliography}{}
\bibitem[Bovy \etal(2012)]{bovy} Bovy, J., Rix, H.-W., Liu, C., \etal, 2012, \apj, 753, 148
\bibitem[Colless \etal(2001)]{2df} Colless, M., Dalton, G., Maddox, S., \etal, 2001, \mnras, 328, 1039
\bibitem[Foreman-Mackey (2014)]{blogpost} Foreman-Mackey, D., 2014, The Histogram (blog post), \url{https://dfm.io/posts/histogram1/}
\bibitem[Foreman-Mackey \etal(2014)]{exopop} Foreman-Mackey, D., Hogg, D.~W., \& Morton, T.~D., 2014, \apj, 795, 64
\bibitem[Kollmeier \etal(2017)]{sdssv} Kollmeier, J.~A., Zasowski, G., Rix, H.-W., \etal, 2017, arXiv e-prints, arXiv:1711.03234
\bibitem[Loredo \& Wasserman(1995)]{loredogrb} Loredo, T.~J., \& Wasserman, I.~M., 1995, \apjs, 96, 261 
\bibitem[Loredo(2004)]{loredo} Loredo, T.~J., 2004, American Institute of Physics Conference Series, 195
\bibitem[York \etal(2000)]{sdss} York, D.~G., Adelman, J., Anderson, J.~E., \etal, 2000, \aj, 120, 1579
\end{thebibliography}

\end{document}
