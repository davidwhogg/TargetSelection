% This file is part of the TargetSelection project
% Copyright 2019 the authors.

% To-dos
% ------
% - make a to-do list
% - make style notes
% - should we have an ``open issues'' section?
% - 

% Style notes
% - 

\documentclass[modern]{aastex62}
% \usepackage{graphicx, xcolor}
% \usepackage[sort&compress]{natbib}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\pc}{\unit{pc}}
\newcommand{\kpc}{\unit{kpc}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}
\newcommand{\given}{\,|\,}

% chemical macros
\newcommand{\abundance}[2]{\mathrm{\left[{#1}/{#2}\right]}}
\newcommand{\alphafe}{\abundance{\alpha}{Fe}}

% text macros
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et al.}}

% margins and headers and spacing changes
\setlength{\parindent}{1.4em} % trust in Hogg
\addtolength{\topmargin}{-0.4in}
\addtolength{\textheight}{0.8in}
\shorttitle{desiderata for target selection}
\shortauthors{hogg \& rix}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
% \graphicspath{ {figures/} }
%\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{\textbf{%
Target selection for spectroscopic surveys:\\
General requirements for statistical and legacy value%
}}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726~Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}
\affil{Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}

\author[0000-0003-4996-9069]{Hans-Walter Rix}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Many new spectroscopic projects are being built and operated
in the areas of
large-scale structure, Milky Way cartography, stellar physics,
exoplanet discovery, and explosive transients.
These must pre-select sources
for spectroscopic observation, from prior imaging (and spectroscopic)
data.
They also have in common that they want their results to be useful for
statistics, population inferences, and legacy use in future projects.
% aims
Here we lay out some general principles for target selection that will
create this legacy value.
% methods
The key idea, from which all principles flow, is that the survey must
be able to produce a computationally tractable
likelihood function for parameters of interest.
This, in turn, recommends that selection be performed such that it is possible
to construct a relatively simple selection function, or a probability
with which any source (real or counter-factual) would have been observed
in a repeat of the survey.
% results
Among our findings and recommendations are the following:
You want to base your selection as much as possible on quantities that
can be reliably and straightforwardly predicted by the relevant
physical models.
You want to avoid having the selection of one source
depend strongly on the properties of \emph{other} sources.
You don't need to know everything about how sources were selected, you
just need to know those aspects of selection that project most strongly
onto the matters of great interest.
If you are presented with survey data that were selected badly,
it is still possible to make a conditional likelihood function that can be
used for inference, but at information-theoretic cost.
\end{abstract}

\keywords{\raggedright
 catalogs
 ---
 methods:~observational
 ---
 methods:~statistical
 ---
 surveys
 ---
 techniques:~spectroscopic
 ---
 telescopes
}

\section{Why target selection matters}

Many projects in conteporary astrophysics proceed by obtaining
wide-field imaging and then selecting targets for spectroscopic
follow-up.
This mode is very old, but it became truly data-intensive with
the \project{2dF} (CITE) and \project{Sloan Digital Sky Survey} (CITE)
projects, which mapped the large-scale structure to test and learn the
cosmological model.
Now there are projects with this character in a range of different
astrophysics sub-fields, including cosmology, Galactic archaeology,
stellar astrophysics, transient characterization, and exoplanet search.
The successes have been legion:
We know the parameters of the cosmological model at percent-level
precision, supernovae of many types have been found and monitored,
hundreds of exoplanets have been spectroscopically confirmed,
and we have three-dimensional maps of the Milky Way in
element abundances and kinematics.
In this \documentname, we would like to emphasize the point that the
quality of the results from these surveys depends both on the
spectroscopic data they take, and equally (or even more importantly)
on the way the spectroscopic targets were selected from the original
imaging data.

Once a project is taking thousands to millions of spectra, the
end-to-end uncertainties in the results it produces tend to be
dominated not by the shot noise (square-root-of-$N$) or sample size,
but rather the ability to understand the selection effects.
That is, to make precise measurements of a population of galaxies or
stars or planets, we need to know what galaxies or stars or planets
were and were not selected for spectroscopic targeting, and what would
have been observed differently if the population of interest had been
different.
That is, we need to understand the \emph{selection function} of the
survey.
We will say more about this below, but the selection function is,
loosely, the probability that a source on the sky would have been
observed spectroscopically if it had been there, conditioned on the
position and physical or observational properties of that source.
The object of this \documentname\ is to elucidate the aspects of
target selection are most relevant to understanding and obtaining a
computationally tractable model of the selection function.

Even when a spectroscopic project is beautifully designed and
executed, it isn't always possible to understand or model this
selection function accurately.
For example, when targets for spectroscopy are selected by hand or by
a human process or human classification, it is often difficult to
figure out what would or would not have been observed in different
(counter-factual) situations.
For another, when each target is chosen in a way that depends strongly
on properties of that target that are difficult to understand within
the context of the physical model, it is difficult to build a useful
model of the selection function.
It might be accurate, but not useful!
For yet another, when one target's selection depends on the properties
of other sources in the vicinity of that target, the selection
function might depend on spatial structures that are hard to model or
hard to manipulate computationally.
Here we develop guidelines for target selection that, if followed,
will improve the chances that it will be possible to build a simple
and accurate selection function.
This, in turn, will improve the chances that the resulting data
will be useful for building a precise model of the relevant source
population.
This, in turn, increases the value of the project for its investigators,
and the legacy value for the community.

In what follows, we make some assumptions about what's important,
scientifically, about the spectroscopic project of interest, and what
is not....

HOGG: SYNTACTICAL REFERENCE foo bar whatever

Here we should state our fundamental assumptions about the reader's goals.
We might mention kinds of surveys and projects that are out of scope!

For example, 2-pt functions are out of scope, because we can't write
down LFs in general.

For another, adaptive follow-up of RV candidates is out of scope, although
many of the same principles will apply.

We should return to these points in the discussion.

\section{The selection function}

Define the nomenclature.

There are data and features and selection as a function of features.

There is an \foreign{a priori} selection function and an \foreign{a posteriori}
selection function. You can only control one of these fully, unless you have near-unit
completeness on your spectral analysis.

Here might be the place to note that the selection function has to something of
which we can ask \emph{counterfactual} questions. It can't just be defined at
the locations of the data.

\section{The likelihood principle}

Bayesian likelihood principle and frequentist optimal estimators.

Relationship between likelihood and forward model. Relationship between
likelihood and noise model.

This section should include the point that the tradition is to divide out
the selection function. That's not good, for divide-by-zero or variance
reasons. There are words in a DFM paper relevant to this.

The divide-by-selection-function methods have the great property that you don't need
to know the SF everywhere. You only need to know it at your data. But then
it can't penalize models that put high weight on locations with near-zero
completeness. That is, it is biased in these regions and wrong.

Here we should also bring up the idea of confounders. If you think the SF
is S(x) but it is actually S(x, w) and you don't track or can't predict w,
then you can't write an accurate LF.

\section{Information theory}

Once you have a LF you can start saying things about information.

There is almost certainly a result (a la Bovy) that we don't need to know
things about the SF that don't project strongly onto our parameters of interest.
Let's make that explicit here, cite Bovy.

When we say you don't have to know everything about the SF, that could be
in space, in time, in Fourier Space, in magnitude, and so on.

\section{Spatial selection functions}

Select on clustering? What's wrong with that?

\section{Doing multiple surveys at once}

Note that if you are doing both MWM and BHM at the same time, there will be
spatial conflicts.

That leads to the selection of one star being affected by properties of the
BHM targets. Can we deal with that? I think yes in this case.

But now think about one MWM target category conflicting with another MWM target
category. That might lead to ``confounder'' situations where the relevant selection
information can't be modeled in any model.

Idea of pre-down-sampling by a fixed, chosen function before starting, to avoid
all spatial conflicts. This has interesting properties. It is a good idea; it isn't
\emph{perfectly} safe (why?), but it is pretty good.

\section{Adaptive strategies}

What changes for, or how do we think about, strategies in which you want to
adaptively deepen certain interesting targets, but cut off uninteresting targets,
while you are operating.

There is a one-target version of this (this target is low metallicity, deepen it).

And there is a target-category version of this (we find that we can't measure WDs
accurately enough to do our science so we drop the category). Or we change the exposure
times we use \emph{during} the survey, based on what we see.

Are these both interesting to our study?

\section{Discussion}

Come back to the assumptions made and talk about what happens when we
adjust or weaken or don't obey these.

For example, what of this discussion carries over to two-point functions?

\acknowledgements It is a pleasure to thank the \project{Milky Way
  Mapper} Collaboration for help with refining these principles.

\facilities{
ESA \project{Gaia},
}

\software{
foo,
bar
}

\begin{thebibliography}{}
\bibitem[De Silva \etal(2015)]{galah} De Silva, G.~M., Freeman, K.~C., Bland-Hawthorn, J., \etal\ 2015, \mnras, 449, 2604 
\end{thebibliography}

\end{document}
